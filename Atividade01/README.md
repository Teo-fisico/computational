# ğŸ“‰ Gradiente Descendente

Este repositÃ³rio apresenta uma implementaÃ§Ã£o simples do algoritmo de **Gradiente Descendente**, um dos mÃ©todos de otimizaÃ§Ã£o mais utilizados em aprendizado de mÃ¡quina. A aplicaÃ§Ã£o Ã© feita sobre umas funÃ§Ãµes quadrÃ¡ticas unidimensional e bidimensional com visualizaÃ§Ãµes grÃ¡ficas que demonstram a convergÃªncia ao mÃ­nimo.

---

## ğŸ“Œ DescriÃ§Ã£o

O **Gradiente Descendente** Ã© um algoritmo iterativo que ajusta os parÃ¢metros de uma funÃ§Ã£o na direÃ§Ã£o oposta ao gradiente, com o objetivo de minimizar seu valor. Ã‰ um componente fundamental de muitos algoritmos de machine learning, como regressÃ£o linear, regressÃ£o logÃ­stica e redes neurais.

A equaÃ§Ã£o de atualizaÃ§Ã£o do gradiente descendente Ã©:

$$
\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)
$$

- $\theta$: parÃ¢metro que queremos otimizar

- $t$: Ã­ndice da iteraÃ§Ã£o

- $\alpha$: taxa de aprendizado (learning rate)

- $\nabla J(\theta_t)$: gradiente da funÃ§Ã£o de custo $J$ em relaÃ§Ã£o a $\theta$

---

## ğŸ“‰ Resultados

A funÃ§Ã£o utilizada para teste Ã© uma parÃ¡bola com mÃ­nimo conhecido. O algoritmo foi executado com diferentes taxas de aprendizado, e observou-se que:

- O algoritmo converge ao mÃ­nimo em poucas iteraÃ§Ãµes com taxa adequada;
- Taxas muito altas causam oscilaÃ§Ãµes ou divergÃªncia;
- Taxas muito baixas tornam a convergÃªncia lenta.

---

## ğŸ“Š GrÃ¡ficos incluÃ­dos

Os seguintes grÃ¡ficos foram gerados e estÃ£o disponÃ­veis na pasta `image1/`:

- **GrÃ¡fico de convergÃªncia questÃ£o 1**  
  Mostra como o valor de `x` se aproxima do mÃ­nimo ao longo das iteraÃ§Ãµes com taxa de aprendizado $\alpha=0,2$.  
  ![GrÃ¡fico de convergÃªncia](./image1/alfame.png)


  AlÃ©m disso, mostra o caminho percorrido pelo algoritmo sobre a curva da funÃ§Ã£o para uma taxa de aprendizado maior ($\alpha=0,9$) e tem uma convergÃªncia oscilante.  
  ![GrÃ¡fico da descida](./image1/alfama.png)

---
- **GrÃ¡fico de convergÃªncia de questÃ£o 2**

Na seguinte figura mostra-se a convergÃªncia lenta, para um ponto inicial perto do mÃ¡ximo global e uma taxa $\alpha=0,1$.
![GrÃ¡fico de convergÃªncia](./image1/q21.png)

Para o mesmo ponto inicial e uma taxa de aprendizado maior nÃ£o converge e dÃ¡ como resultado errado (ao mÃ¡ximo global). 

![GrÃ¡fico de convergÃªncia](./image1/q21b.png)


Para um ponto inicial $x=2$ e uma taxa de aprendizado $\alpha=0,1$ a convergÃªncia Ã© rÃ¡pida.
![GrÃ¡fico de convergÃªncia](./image1/q21c.png)

Na mesma conciÃ§Ã£o inical e uma taxa maior converge rÃ¡pidamente no mÃ­nimo do lado oposto. 

![GrÃ¡fico de convergÃªncia](./image1/q21d.png)


Para determinar o valor mÃ­nimo global depende de taxa de aprendizado e ponto inicial. 

---

- **GrÃ¡fico de convergÃªncia de questÃ£o 3**

As figuras mostram-se as convergÃªncias dos mÃ­nimos dependentes do taxa de aprendizado.


![GrÃ¡fico de convergÃªncia](./image1/31.png)

![GrÃ¡fico de convergÃªncia](./image1/32.png)

---
- **GrÃ¡fico de convergÃªncia de questÃ£o 4**

Na seguinte figura tambÃ©m apresenta-se a dependÃªncia da taxa de aprendizado e ponto inicial para converger a um mÃ­nimo local ou global.

![GrÃ¡fico de convergÃªncia](./image1/min_glo.png)

Finalmente, a figura mostra a convergÃªncia em funÃ§Ã£o de Ã©pocas e existendo uma estabilidade para maiores que Ã©poca=20. 

![GrÃ¡fico de convergÃªncia](./image1/conv.png)
