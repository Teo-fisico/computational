# ğŸ“‰ Gradiente Descendente

Este repositÃ³rio apresenta uma implementaÃ§Ã£o simples do algoritmo de **Gradiente Descendente**, um dos mÃ©todos de otimizaÃ§Ã£o mais utilizados em aprendizado de mÃ¡quina. A aplicaÃ§Ã£o Ã© feita sobre umas funÃ§Ãµes quadrÃ¡ticas unidimensional e bidimensional com visualizaÃ§Ãµes grÃ¡ficas que demonstram a convergÃªncia ao mÃ­nimo.

---

## ğŸ“Œ DescriÃ§Ã£o

O **Gradiente Descendente** Ã© um algoritmo iterativo que ajusta os parÃ¢metros de uma funÃ§Ã£o na direÃ§Ã£o oposta ao gradiente, com o objetivo de minimizar seu valor. Ã‰ um componente fundamental de muitos algoritmos de machine learning, como regressÃ£o linear, regressÃ£o logÃ­stica e redes neurais.

A equaÃ§Ã£o de atualizaÃ§Ã£o do gradiente descendente Ã©:

$$
\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)
$$

- $\theta$: parÃ¢metro que queremos otimizar

- $t$: Ã­ndice da iteraÃ§Ã£o

- $\alpha$: taxa de aprendizado (learning rate)

- $\nabla J(\theta_t)$: gradiente da funÃ§Ã£o de custo $J$ em relaÃ§Ã£o a $\theta$

---

## ğŸ“‰ Resultados

A funÃ§Ã£o utilizada para teste Ã© uma parÃ¡bola com mÃ­nimo conhecido. O algoritmo foi executado com diferentes taxas de aprendizado, e observou-se que:

- O algoritmo converge ao mÃ­nimo em poucas iteraÃ§Ãµes com taxa adequada;
- Taxas muito altas causam oscilaÃ§Ãµes ou divergÃªncia;
- Taxas muito baixas tornam a convergÃªncia lenta.

---

## ğŸ“Š GrÃ¡ficos incluÃ­dos

Os seguintes grÃ¡ficos foram gerados e estÃ£o disponÃ­veis na pasta `docs/`:

- **GrÃ¡fico de convergÃªncia (`convergencia_x.png`)**  
  Mostra como o valor de `x` se aproxima do mÃ­nimo ao longo das iteraÃ§Ãµes.  
  ![GrÃ¡fico de convergÃªncia](docs/convergencia_x.png)

- **GrÃ¡fico da descida na funÃ§Ã£o (`descida_funcao.png`)**  
  Mostra o caminho percorrido pelo algoritmo sobre a curva da funÃ§Ã£o.  
  ![GrÃ¡fico da descida](docs/descida_funcao.png)

---
